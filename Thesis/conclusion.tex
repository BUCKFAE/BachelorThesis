%!TEX root = thesis.tex

\chapter{Conclusion}
\label{ch:conclusion}

While simple rules like picking the most damaging move are sufficient to defeat 
\textit{RandomPlayer} most of the time, more complex routines are required 
in order to rival human players. 

Previously existing search-based implementations managed to yield good results against 
baseline agents but are outperformed by the current state of the art DQN-Agent 
described by~\cite{Huang_Lee_2019}. 
Additionally, a way to implement long term planning by modifying the MiniMax-Algorithm
was proposed. Chapter \ref{ch:evaluation} revealed the difficulty in proper evaluation
of agents created by the influence of \ac{RNG}. 

\textit{HerrGewitter} failing to rival \textit{pmariglia} is likely caused by its inferior
item prediction algorithm and handling of edge cases. Nevertheless, the agent is capable
of defeating less experienced humans. Future work will address the flaws described in
section \ref{sec:HerrGewitter}. Additionally, a refined dynamaxing-routine and improved
predictions of enemy actions are worth investigating.

Pointed out by~\cite{Huang_Lee_2019}, the DQN-Agent might be improved
by introducing \grqq different network architectures or the addition
of a recurrent element like an LSTM to better model human memory during the 
course of a game\grqq. Experiments combining supervised learning and
reinforcement learning similar to the \textit{StarCraft II} AI \textit{AlphaStar} 
\todo{https://deepmind.com/blog/article/alphastar-mastering-real-time-strategy-game-starcraft-ii}
developed by \textit{DeepMind} may improve the agent even further. 