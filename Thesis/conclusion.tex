%!TEX root = thesis.tex

\chapter{Conclusion}
\label{ch:conclusion}
\todo{@Samuel: Conclusion bitte lesen}
While simple rules like picking the most damaging move are sufficient to defeat 
\textit{RandomPlayer} most of the time, more complex routines are required 
in order to rival human players. 

Previously existing search-based implementations managed to yield good results against 
baseline agents but are outperformed by the current state of the art DQN-Agent 
described by~\cite{Huang_Lee_2019}. 
Additionally, a way to implement long term planning by modifying the \textit{MiniMax}-Algorithm
was proposed. The heavy influence of \ac{RNG} results in some games being unbalanced favoring one
of the two players heavily and therefore, a large quantity of games is required to properly 
compare different agents.

\textit{HerrGewitter} failing to rival \textit{pmariglia} is likely caused by its inferior
item prediction algorithm and handling of edge cases. Nevertheless, the agent is capable
of defeating less experienced humans. Further possible improvements to the agent include
item prediction and the refinement of the \textit{MiniMax}-Algorithm by including status
and boost. Lastly, a refined dynamaxing-routine and improved
predictions of enemy actions are worth investigating.

Pointed out by~\cite{Huang_Lee_2019}, the DQN-Agent might be improved
by introducing \grqq different network architectures or the addition
of a recurrent element like an LSTM to better model human memory during the 
course of a game\grqq. Experiments combining supervised learning and
reinforcement learning similar to the \textit{StarCraft II} AI~\cite{OpenAI:AlphaStar}
developed by \textit{DeepMind} may improve the agent even further. 