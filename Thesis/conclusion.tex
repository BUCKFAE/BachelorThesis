%!TEX root = thesis.tex

\chapter{Conclusion}
\label{ch:conclusion}
While simple rules like picking the most damaging move are sufficient to defeat 
\textit{RandomPlayer} most of the time, more complex routines are required 
in order to rival human players. Existing search-based agents lack proper long
term planning as the large amount of possible states limits the amount of turns
that can be calculated in advance. This thesis introduces a combination 
of existing \textit{Rule Based}- and \textit{MiniMax}-Agents in order to
create and execute a match plan. Proposed agents were able to defeat baseline opponents
reliably and are challenging to defeat, even for more experienced humans. \\
\cite{Huang_Lee_2019} developed a DQN-Agent that makes use of embeddings which is 
able to defeat the search-based agent of~\cite{Github:pmariglia-showdown}. 
Evaluation of agents for \textit{Pok√©mon Showdown} needs to be done with regard to
the large amount of \ac{RNG} that results in one player often having an objectively
stronger team than his opponent.
\textit{HerrGewitter} failing to rival \textit{pmariglia} is likely caused by its inferior
item prediction algorithm and handling of edge cases. 

Further possible improvements to \textit{HerrGewitter} include
item prediction and the refinement of the \textit{MiniMax}-Algorithm by including status
and boosts. Lastly, a refined dynamaxing-routine and improved
predictions of enemy actions are worth investigating.

Pointed out by~\cite{Huang_Lee_2019}, the DQN-Agent might be improved
by introducing \grqq different network architectures or the addition
of a recurrent element like an LSTM to better model human memory during the 
course of a game\grqq. Experiments combining supervised learning and
reinforcement learning similar to the \textit{StarCraft II} AI~\cite{OpenAI:AlphaStar}
developed by \textit{DeepMind} may improve the agent even further. 

The advantage of a rule based approach is the capability to quickly adapt rules to 
other game modes, as no re-training is required. Therefore, our approach can be 
quickly applied in newer games which are released by Nintendo on a regular basis.