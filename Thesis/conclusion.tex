%!TEX root = thesis.tex

\chapter{Conclusion}
\label{ch:conclusion}

While the current state of the art DQN-Agent described in~\autocite{Huang_Lee_2019}
manages to outperform all currently available search-based agents, this approach
has the drawback that the model needs to be retrained for each possible battle
format. As pointed out by~\autocite{Huang_Lee_2019}, further work might improve
the agent by introducing \grqq different network architectures or the addition
of a recurrent element like an LSTM to better model human memory during the 
course of a game\grqq. Experiments combining supervised learning and
reinforcement learning similar to the \textit{StarCraft II} AI \textit{AlphaStar} 
\todo{https://deepmind.com/blog/article/alphastar-mastering-real-time-strategy-game-starcraft-ii}
developed by \textit{DeepMind} may improve the agent even further. \\
I will continue work on the rule based agent created in this thesis and hope to
be able to defeat the current state-of-the-art search based agent of~\autocite{Github:pmariglia-showdown}
once my implementation contains the features it is currently lacking described in section 
\ref{sec:HerrGewitter}.
